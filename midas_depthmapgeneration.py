# -*- coding: utf-8 -*-
"""MIDAS_DepthMapGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jrAip_MTPsFtWkwVOixjJWPp_YOhFVkO

Connect to either AWS instance or Google Colab T4 / A100 runtime
"""

# from google.colab import drive
# drive.mount('/content/drive')

import json
import torch
import os
import numpy as np
import requests
from pathlib import Path
import hashlib
from transformers import pipeline
import io
import json
import cv2
import pandas as pd
import torch
from datasets import load_dataset
from PIL import Image
from tqdm import tqdm
import re
from collections import defaultdict

# retreived from hugging face website, LAION SG dataset
ds = load_dataset("mengcy/LAION-SG", split = "train")

ds_v = load_dataset("mengcy/LAION-SG", split = "validation")
print(ds_v)

print(ds)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# Load MiDaS model, code rferenced from https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/intelisl_midas_v2.ipynb#scrollTo=7fffe79b
midas = torch.hub.load("intel-isl/MiDaS", "DPT_Large")
midas.to(device)
midas.eval()

midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
transform = midas_transforms.dpt_transform  # for DPT_Large

def compute_depth_64x64_from_rgb(img_rgb: np.ndarray, save_prefix: str):

    input_batch = transform(img_rgb).to(device)

    with torch.no_grad():
        prediction = midas(input_batch)

        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=img_rgb.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze(0).squeeze(0)

    depth = prediction.cpu().numpy().astype(np.float32)

    # downsample to 64x64 iamges
    depth_64 = cv2.resize(depth, (64, 64), interpolation=cv2.INTER_AREA)

    #normalize to [0,1]
    d_min = float(depth_64.min())
    d_max = float(depth_64.max())
    if d_max > d_min:
        depth_norm = (depth_64 - d_min) / (d_max - d_min)
    else:
        depth_norm = np.zeros_like(depth_64, dtype=np.float32)

    depth_norm = depth_norm.astype(np.float32)

    # save .npy files
    npy_path = os.path.join(DEPTH_NPY, f"{save_prefix}_depth64.npy")
    np.save(npy_path, depth_norm)

    # Optional save as PNG for checking the images generated, saves storage to be optional
    # png_path = os.path.join(DEPTH_PNG_64x64, f"{save_prefix}_depth64.png")
    # depth_png = (depth_norm * 255.0).clip(0, 255).astype(np.uint8)
    # cv2.imwrite(png_path, depth_png)

    return npy_path

#Directories for Images
# BASE_DIR = "/content/drive/MyDrive/laion_sg_subset"  # for running on Colab, my GDrive
BASE_DIR = "laion_sg_subset"  # for running on AWS
IMAGES = os.path.join(BASE_DIR, "images")
DEPTH_NPY = os.path.join(BASE_DIR, "depth_64_npy")
DEPTH_PNG_64x64 = os.path.join(BASE_DIR, "depth_64_png")

os.makedirs(BASE_DIR, exist_ok=True)
os.makedirs(IMAGES, exist_ok=True)
os.makedirs(DEPTH_NPY, exist_ok=True)
os.makedirs(DEPTH_PNG_64x64, exist_ok=True)

# Generating depth maps and npy

TOTAL = len(ds)
CHUNK_SIZE = 10000          # 10k per chunk
MAX_TOTAL = 100000          # stop after 100k images

# generate depth maps & images per 10,000 imgs in 1 chunk, 10 chunks total (0-9)
START_CHUNK_ID = 0           # start at chunk0
NUM_CHUNKS = 9               # 9 more chunks

print(f"Total dataset size: {TOTAL}")
print(f"Will process up to {MAX_TOTAL} images starting from chunk {START_CHUNK_ID}")

for CHUNK_ID in range(START_CHUNK_ID, START_CHUNK_ID + NUM_CHUNKS):
    start = CHUNK_ID * CHUNK_SIZE
    end = min(start + CHUNK_SIZE, TOTAL, MAX_TOTAL)

    if start >= TOTAL or start >= MAX_TOTAL:
        print(f"Chunk {CHUNK_ID}: start={start} beyond limit, stopping.")
        break

    print(f"\n=== Starting CHUNK_ID={CHUNK_ID}, indices [{start}, {end}) ===")

    subset = ds.select(range(start, end))

    records = []
    processed = 0

    for example in tqdm(subset, total=len(subset)):
        img_id = example["img_id"]
        name = example["name"]
        url = example["url"]
        caption = example["caption_ori"]
        expected_npy_path = os.path.join(DEPTH_NPY, f"{img_id}_depth64.npy")

      # In case of time out for reruns: if depth already exists, just record and skip
        if os.path.exists(expected_npy_path):
            img_path = os.path.join(IMAGES, name)

            records.append({
                "img_id": img_id,
                "name": name,
                "caption_ori": caption,
                "score": example.get("score", None),
                "url": url,
                "image_path": img_path,
                "depth_npy": expected_npy_path,
                "items": json.dumps(example["items"]),
                "relations": json.dumps(example["relations"])
            })
            continue

        #download image
        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            img = Image.open(io.BytesIO(resp.content)).convert("RGB")
        except Exception as e:
            print(f"Skipping img_id={img_id} due to download error: {e}")
            continue

        # save image
        img_path = os.path.join(IMAGES, name)
        try:
            img.save(img_path)
        except Exception as e:
            print(f"skipping image img_id={img_id}, dataset url doesn't work: {e}")
            continue

        # compute depth of img
        img_rgb = np.array(img)
        try:
            npy_path = compute_depth_64x64_from_rgb(
                img_rgb,
                save_prefix=str(img_id)# image id
            )
        except Exception as e:
            print(f"Skipping image img_id={img_id}, got depth error: {e}")
            continue

        # record data to csv about image for reference
        records.append({
            "img_id": img_id,
            "name": name,
            "caption_ori": caption,
            "score": example.get("score", None),
            "url": url,
            "image_path": img_path,
            "depth_npy": npy_path,
            # "depth_png": png_path, # not necessary
            "items": json.dumps(example["items"]),
            "relations": json.dumps(example["relations"])
        })

        processed += 1
        if processed % 200 == 0:
            print(f"  [chunk {CHUNK_ID}] processed {processed} images")

    # 5 save this chunk’s CSV to Drive
    chunk_index_path = os.path.join(
        BASE_DIR,
        f"laion_sg_depth_index_chunk_{CHUNK_ID:04d}.csv"
    )
    df = pd.DataFrame(records)
    df.to_csv(chunk_index_path, index=False)
    print(f"--- Finished CHUNK_ID={CHUNK_ID}, saved {len(records)} rows to {chunk_index_path} ---")

print("\nAll requested chunks attempted.")

# Confirm csv file path
chunk_index_path = os.path.join(
    BASE_DIR,
    f"laion_sg_depth_index_chunk_{CHUNK_ID:04d}.csv"
)
df = pd.DataFrame(records)
df.to_csv(chunk_index_path, index=False)
chunk_index_path

# checking how many files are in each chunk
# folder_path = "/content/drive/MyDrive/laion_sg_subset/depth_64_npy"
folder_path = "Depth Maps/laion_sg_subset/depth_64_npy"

# Dictionary hold counts per chunk
chunk_counts = defaultdict(int)

# extract the # prefix from filenames
pattern = re.compile(r"^(\d+)")

for fname in os.listdir(folder_path):
    if fname.endswith(".npy"):
        match = pattern.match(fname)
        if match:
            num = int(match.group(1))
            chunk = (num // 10000)
            chunk_counts[chunk] += 1

print("Counts per 10k chunk:")
for chunk in sorted(chunk_counts.keys()):
    print(f"Chunk {chunk} (files {chunk*10000}–{chunk*10000+9999}): {chunk_counts[chunk]} files")

"""# Just checking with one image what npy file looks like"""

import matplotlib.pyplot as plt

sample = records[0]
depth_arr = np.load(sample["depth_npy"])
plt.imshow(depth_arr, cmap="inferno")
plt.colorbar()
plt.title(f"img_id={sample['img_id']} | {sample['caption_ori'][:30]}...")
plt.show()

depth = np.load("laion_sg_subset/depth_64_npy/1_depth64.npy")
print(depth.shape)
print(depth.dtype)
print(depth.min(), depth.max())

